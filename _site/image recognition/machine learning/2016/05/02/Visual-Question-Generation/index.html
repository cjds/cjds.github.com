<html>
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

	<link rel="stylesheet" type="text/css" href="/stylesheets/app.css">
<!--	<link href='http://fonts.googleapis.com/css?family=Roboto+Slab:300' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Playfair+Display' rel='stylesheet' type='text/css'>-->
	<script type="text/javascript" href="/js/foundation.min.js"></script>
		<script type="text/javascript" src="/js/jquery.js"></script>

	<script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js?autoload=false"></script>
		<link href='/css/prettify.css' rel='stylesheet' type='text/css'>
	<title>Visual Question Generation</title>
	<link href='http://fonts.googleapis.com/css?family=Lora' rel='stylesheet' type='text/css'>
<link href='https://fonts.googleapis.com/css?family=Raleway' rel='stylesheet' type='text/css'>

  


</head>
<style>
@import url(http://fonts.googleapis.com/css?family=Roboto+Slab:100);


@font-face {
    font-family: Mohave;
    src: url('/fonts/Mohave.otf');
}

ol li{
 	font-family: "Lora","Helvetica",Arial,sans-serif;
 	font-size: 1.1em;
 	color:#344;
 }

 ul li{
 	font-family: "Lora","Helvetica",Arial,sans-serif;
 	font-size: 1.1em;
 	color:#344;
 }
 p{
 	font-family: "Lora","Helvetica",Arial,sans-serif;
 	font-size: 1.1em;
 	color:#344;
 	text-align:justify;
 	margin-top:10px;
 	margin-bottom: 20px;
 }



.body-code{
	position: absolute;
	bottom: 0;
	top:40px;
	width:100%;
}
.fun-heading {
	height:100%;

	margin-top: 0px;
/*	box-shadow: -10px 0px 5px 0px rgba(0, 0, 0, 0.2);;*/

}

hr { 
	display: block; height: 1px;
    border: 0; border-top: 1px solid #000;
    margin: 1em 0; padding: 0; opacity: 0.2;}

.fun-heading ul {
	margin-left: 0;
}
.fun-heading ul a li{
	text-align: left;
	font-family: 'Verdana',sans-serif;
	font-weight: 500;
	font-size: 1em;
	line-height: 2em;
	list-style: none;
	color: #666;
	margin: 10px;
}
.fun-heading ul a:hover{
	text-decoration: underline;
}

.fun-heading h1{
	font-family: 'Roboto Slab',serif;
	font-weight: 100;
	font-size: 3.5em;	
	color: #000;
	opacity: 0.3;
	margin-top: -0.1em;
}

 h1,h2,h3,h4,h5,h6{
 	/*font-weight: 900*/
 }

 h1{
 	margin-top: 1rem;
 }

h2{
	color:#444;
	font-family: 'Raleway', serif;
	font-size: 1.8em;
}

h3{
	font-size: 1.4em;
}



 hr{
 	margin:0px;
 	margin-bottom: 5px;
 }


 hr:not(:first-child){
 	margin: auto;
 	margin-top: 40px;
 	margin-bottom: 40px;
 	width: 160px;
 }

.top-bar{
	background-color:#fff;
	color:#000;
}

.top-bar .name h1 a{
	color:#333;
	font-family:Mohave;
	font-size: 1.4rem;
}


.top-bar .name h1 a:hover{
	color:#b22;
}


.top-bar .right li a{
	background: #fff;
	color:#333;
	font-family:Mohave;
	font-size: 1.4rem;
}

.top-bar-section li:not(.has-form) a:not(.button){
	background: #fff;
}

.top-bar .right li a:hover{
	text-decoration:underline;
}
.top-bar .toggle-topbar.menu-icon a{
	color:#333;
}


/*
.top-bar{
	background-color:#fff;
	position: absolute;
	top:0;
	box-shadow: 0px 2px 5px 5px rgba(0, 0, 0, 0.2);
	z-index: 2;
	width: 100%;
	height:40px;
}
 .top-bar >ul >li >h1 > a{
 	font-weight: 900 !important;
 	color:#000 !important;

 }

 code{
 	overflow: auto;

 	font-family: "Verdana","Helvetica",Arial,sans-serif;
 	color:#555;
	background-color: #eee;
	padding:0px 4px 3px 4px;
	letter-spacing: 1px;
	line-height: 130%;
	word-spacing: 2px;
	margin: 4px;
	font-weight: normal;
	font-size: 0.9em;
 }

 pre code{
 	padding:0px;
	margin: 0px;
	word-wrap: initial;
	background:none !important;
	
 }

pre{
	background-color: #eee;
	border-left:solid 5px #f00000 !important;
	padding:6px;
	margin: 6px;
	overflow-x:auto;
	overflow-y: hidden;
 }
*/

pre{
	background-color: #333;
	padding:2px;
	margin: 2px;
	overflow-x:auto;
	overflow-y: hidden;
 }

 .big img{
 border-top: solid 1px #ccc;
 width:100%;
 	margin: 0px auto;
 	box-shadow: 0px 5px 5px #aaa;
 }

 .image img{
 	margin: 10px auto 30px;
 	display: block;
 }

 .image p{
	margin: 0px auto;
 }

 .caption p{
 	margin: 0px auto;
 	display: block;
 	font-size: 0.8em;
 	text-align: center;
 	margin-bottom: 25px;
 }


 .post-div{
 	padding: 25px;
 	margin-top: 50px;
 	border-top: solid 1px #ccc;
 box-shadow: 0px 5px 5px #ccc;	
 }

 .post-div > h1{
 	margin-top: 10px;
 	margin-bottom: 1px;
 	font-family: 'Mohave';
 	text-align: center;
 	font-weight: 300

 }

 .top-bar .toggle-topbar a{
 	font-size: 1.4em;
 	color:#333;
 	font-weight: 300;
 }

</style>
<body>

<nav class="top-bar" data-topbar >
  <ul class="title-area">
    <li class="name">
      <h1><a href="http://carlsaldanha.com" style='text-align:left' >Carl Saldanha</a></h1>
    </li>
        <li class="toggle-topbar" style='font-family:Mohave;color:#666'><a href="#">Menu</a></li>

  </ul>

  <section class="top-bar-section">
    
    <ul class="right">

      <li><a href="http://carlsaldanha.com/resume">Resume</a></li>
      <li><a href="http://carlsaldanha.com/projects">Projects</a></li>

      <li><a href="http://cjds.github.io/">Blog</a></li>
      </li>
    </ul>

  </section>
</nav>
<div class='large-6 large-offset-3 medium-8 medium-offset-2 small-12 post-div'>
<h1>Visual Question Generation</h1>

<hr>
<h3>Motivation</h3>

<p>The problem of algorithm truly understanding a scene is still unsolved and current databses with images and questions are still unsolved. Perhaps if we can train on a much larger series of questions, we will be able to acheive better results. For this reason, I propose creating a artificial dataset of natural language questions about images.</p>

<h3>Introduction</h3>

<p>There aew a string of successes in using deep learning for object recognition and image captioning among other tasks.
However, algorithms that can perform complete scene understanding is still unsolved. Our focus is on generating large datasets of questions for images to train on. The idea behind this, is that if an algorithm could ask a series of questions about images that it's seeing, it could potentially one day learn how to answer them.</p>

<p>The sapce for asking questions however, is much larger than the space of answers. (For any given question there are just a few non-trivial). However, the number of questions one can ask about a given scene is nearly infinite.</p>

<h3>Related Work</h3>

<p>There is not any directly applicable work in the   not been attempts to learn the question space from images, there have been attempts at creating a series of questions about bodies of text.</p>

<p>In Computer-Aided Generation of Multiple-Choice Tests[3], the authors picked the key nouns in the paragraph and and then use a regular expression to generate the question.
In the image domain, there have been attempts at visual question generation and image understanding. To do this there have been multiple datasets created, though they're overall size is small when comparing to datasets like MSCOCO and ImageNet</p>

<p>Visual Madlibs[6]: In Visual madlibs people generate fill in the blank question answer pairs which can be used in deep learning models to understand more about systems</p>

<p>VQA[1] is a dataset that has a series of non-trivial questions about the images that are contained within. They also contain Mulitiple Choice along with Open Ended question
Visual Genome Project is a similar dataset that contains a list of relationships between objects in images, that potentially could benefit large scale scene understanding.</p>

<h3>Algorithm</h3>

<p>There are three methods that we are going to discuss. We are using the VQA dataset as a database to train our models. As such, we have 3 training questions for each question.</p>

<p>We used a Bidirectional LSTM to model our language 2 * 2 * 512. The output of the LSTM is always a bag of words feature that the user means to ask. The final layer of the LSTM is equal to the longest sentence in the input. A special start and stop symbol is added to the input bag of words. The Convolutional Features are selected by trying to align with the words in the question to develop a model as to when to ask which question. We are trying to align the words in the questions to the output of the images similar to the method in Deep Visual Alignments, Karpathy et. al [2]</p>

<p>The first is a baseline test of passing a series of Convolution based features to an LSTM and training it to output a bag of words which forms the sentence. The second method uses the a combination of image features and a 1-of-k representation of the first word input to the LSTM which then generates the output. The second uses a 1 * 1 *1024 dimension Word2Vec output which is then multiplied with the CNN features to create an input to the LSTM.</p>

<h3>Network Description</h3>

<p>I used the VGG-net 16 as a CNN to generate the image features. The features are connected from the last fully connected layer. This is 4096 dimensional feature vector which is fused with the Word2Vec input using element-wise multiplication, similar to that seen in the VQA dataset. With the bag of words encoding, the input is transformed into a 1024 vector with one fully connected layer which is then input into the LSTM</p>

<p><img src='http://cjds.github.io//assets/2016-05-02/network.png' style="margin-top:50px"/></p>

<p>I used stochastic gradient descent to backpropogate in the Convolutional network and RMS Prop in the RNN. The size of the final dataset used was 50,000 images, each with 3 captions for each question. The training was run for 100 epochs and the training error was calculated at every 1000 iterations. The images were split into batches of 16 for minimizing training time.</p>

<p><img src='http://cjds.github.io//assets/2016-05-02/training error .png' style="margin-top:50px"/>
The work was done in Torch and can be found <a href="here(latest%20code%20coming%20soon">https://github.com/cjds/WhatIsTheMan</a>)</p>

<h3>Results</h3>

<p>The network was trained on the Real images section of VQA and under the
The results were mixed with all images generating questions, but the variation in the questions not being large. Over 60% of the questions generated are in the form "What is the man _____", which overall while being a valid question lacks the depth to create a database from.</p>

<p>This is the spread of the number of words in each of the three methods compared with the original dataset is here. We are trying to model the distribution of sentence lengths in blue. As can be seen there is a marked difference between the Word2Vec model vs the image alone, which is suggestive that Word2Vec might be more accurate.
<img src='http://cjds.github.io//assets/2016-05-02/sentence distribution.png' style="margin-top:50px"/></p>

<p>I, then took 50 questions from each of the 4 sets and rated whether the question was indeed plausible for the images.The results are shown below. (Question represents the original dataset, which at 49 is always plausible)</p>

<p><img src='http://cjds.github.io//assets/2016-05-02/number.png' style="margin-top:50px"/></p>

<p>Again, the Word2Vec method beat out the competition to acheive a much higher score than all others suggesting that this feature encoding holds promise for more work in the future</p>

<h3>Conclusions</h3>

<p>The bag-of-words method clearly performs the best of all the methods, however to ask questions due to their very nature new methods will have to be developed. This is insufficient to capture the space of questions that one can truly ask. Future work may go into how to use the bag of words to cause enough jitter to still ask sensible questions and artifically increase size that way</p>

<h3>References</h3>

<p>[1]Antol, Stanislaw, et al. "VQA: Visual question answering." Proceedings of the IEEE International Conference on Computer Vision. 2015.</p>

<p>[2]Karpathy, Andrej, and Li Fei-Fei. "Deep visual-semantic alignments for generating image descriptions." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.</p>

<p>[3]Mitkov, R., &amp; Ha, L. A. (2003, May). Computer-aided generation of multiple-choice tests. In Proceedings of the HLT-NAACL 03 workshop on Building educational applications using natural language processing-Volume 2 (pp. 17-22). Association for Computational Linguistics.</p>

<p>[4]Aldabe, Itziar, et al. "Arikiturri: an automatic question generator based on corpora and nlp techniques." Intelligent Tutoring Systems. Springer Berlin Heidelberg, 2006.</p>

<p>[5]Li Fei-Fei, et al. "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"</p>

<p>[6]Alexander Berg, Tamara L. Berg, et al. "Visual Madlibs: Fill in the blank Image Generation and Question Answering" arXiv preprint 2015</p>

<hr>
<i>02 May 2016 </i>
</div>
<script src="/js/foundation/foundation.js"></script>
  <script src="/js/foundation/foundation.topbar.js"></script>

  <script>
    $(document).foundation();
  </script>
  
<script type='text/javascript'>
$( document ).ready(function(){


  // Add pretty print to all pre and code tags.
  $('pre, code').addClass("prettyprint");

  // Remove prettify from code tags inside pre tags.
  $('pre code').removeClass("prettyprint");

  // Activate pretty presentation.
  prettyPrint();
});
</script>
</body>
</html>